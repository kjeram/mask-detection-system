{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "from tqdm import tqdm\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Dataset loader --#\n",
    "\n",
    "def fetch_files(dir:str, filetype:list, arr:list = []):\n",
    "    with os.scandir(dir) as content:\n",
    "        for item in content:\n",
    "            if os.path.isdir(dir + '/' + item.name):\n",
    "                arr = fetch_files(dir + '/' + item.name, filetype, arr)\n",
    "            elif item.name.split('.')[-1] in filetype:\n",
    "                arr.append(item)\n",
    "    return arr\n",
    "\n",
    "def load_dataset(arr:list):\n",
    "    for i in arr:\n",
    "        assert type(i) == os.DirEntry\n",
    "    data = [cv2.cvtColor(cv2.imread(x.path), cv2.COLOR_BGR2RGB) for x in arr]\n",
    "    label = [x.path.replace(x.name, '')[:-1].split('/')[-1] for x in arr]\n",
    "    return data, label\n",
    "\n",
    "#-- Data Processer --#\n",
    "\n",
    "def center_crop_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    aspect = h/w\n",
    "    if aspect > 1:\n",
    "        offset = int(np.round((h / 2) - (w / 2)))\n",
    "        return img[offset:w + offset, 0:w]\n",
    "    else:\n",
    "        offset = int(np.round((w / 2) - (h / 2)))\n",
    "        return img[0:h, offset:h + offset]\n",
    "\n",
    "def process_images(images, image_size):\n",
    "    cropped_images = [center_crop_image(img) for img in images]\n",
    "    resized_images = [cv2.resize(img, (image_size, image_size), cv2.INTER_AREA) for img in cropped_images]\n",
    "    return resized_images\n",
    "\n",
    "def data_augmenter(data, label, rot=0, step=1, flip=True):\n",
    "    \"\"\"Augments the data.\n",
    "    \n",
    "    ### Parameters\n",
    "    data : array_like\n",
    "        Data to be augmented.\n",
    "    label : array_like\n",
    "        Label which corresponding to the data.\n",
    "    rot : integer\n",
    "        Rotataion degree clockwise and counterclockwise.\n",
    "    step : integer\n",
    "        Number of steps to be rotated within the rot range.\n",
    "    flip : bool\n",
    "        Flips and doubles the data, including all rotations.\n",
    "    \n",
    "    Calculation used for rot and step params: range(- rot, rot + 1, step)]\n",
    "    \n",
    "    ### Returns\n",
    "    data : list\n",
    "        The augmented data.\n",
    "    label : list\n",
    "        List of the labels corresponding to the augmented data.\"\"\"\n",
    "    data_pp, label_pp = [], []\n",
    "    rotation = [x for x in range(- rot, rot + 1, step)]\n",
    "    for d, l in zip(data, label):\n",
    "        for r in rotation:\n",
    "            frame = imutils.rotate(d, r)\n",
    "            data_pp.append(frame)\n",
    "            label_pp.append(l)\n",
    "            if flip:\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                data_pp.append(frame)\n",
    "                label_pp.append(l)\n",
    "    return data_pp, label_pp\n",
    "\n",
    "#-- Data visualizer --#\n",
    "\n",
    "def visualizer(x, y=None, grid=None, font=None):\n",
    "    if not font:\n",
    "        font = {'font.family': 'Arial', 'font.size' : 12}\n",
    "    rcParams.update(font)\n",
    "    if not grid or 1 in grid:\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        plt.tick_params(axis='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "        plt.imshow(x, cmap=\"Greys\")\n",
    "        if y: plt.title(y)\n",
    "    else:\n",
    "        fig, axes = plt.subplots(grid[0], grid[1],figsize=(10,10))\n",
    "        for row in axes:\n",
    "            for axe in row:\n",
    "                axe.tick_params(axis='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "                r = np.random.randint(len(x))\n",
    "                axe.imshow(x[r], cmap=\"Greys\")\n",
    "                if y: axe.set_title(y[r])\n",
    "                #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'B:/dataset/size224_seed1_conf0.7_limit_onlymask'\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "rot = 16\n",
    "step = 8\n",
    "flip = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = load_dataset(fetch_files(dataset, filetype=['jpg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_images(data, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer(data, label, grid=(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_rem, y_train, y_rem = train_test_split(data, label, train_size=0.8)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_augmenter(X_train, y_train, rot, step, flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X\n",
    "X_train = np.array(X_train).reshape(-1, image_size, image_size, 3)\n",
    "X_test = np.array(X_test).reshape(-1, image_size, image_size, 3)\n",
    "X_valid = np.array(X_valid).reshape(-1, image_size, image_size, 3)\n",
    "\n",
    "# Categorize y\n",
    "unique_labels = []\n",
    "[unique_labels.append(x) for x in label if x not in unique_labels]\n",
    "y_train = np.array([unique_labels.index(x) for x in y_train])\n",
    "y_test = np.array([unique_labels.index(x) for x in y_test])\n",
    "y_valid = np.array([unique_labels.index(x) for x in y_valid])\n",
    "\n",
    "# encode the y labels\n",
    "categories = len(unique_labels)\n",
    "y_train = to_categorical(y_train, categories)\n",
    "y_test = to_categorical(y_test, categories)\n",
    "y_valid = to_categorical(y_valid, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, AveragePooling2D\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(MobileNetV2(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False))\n",
    "    # model.add(Flatten())\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(7,7)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(categories, 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet (no transfer-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # AlexNet\n",
    "# def init_model():\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # 1st Convelutional Layer\n",
    "#     model.add(Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), padding='same',\n",
    "#     input_shape=(image_size,image_size,3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#     # 2nd Convelutional Layer\n",
    "#     model.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#     # 3rd Convelutional Layer\n",
    "#     model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     # 4th Convelutional Layer\n",
    "#     model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "\n",
    "#     # 5th Convelutional Layer\n",
    "#     model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#     # Fully connected layer\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(4096))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     # 2nd Connected Layer\n",
    "#     model.add(Dense(4096))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     # 3nd Connected Layer\n",
    "#     model.add(Dense(1000))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "\n",
    "#     # Output\n",
    "#     model.add(Dense(categories))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "#     return model\n",
    "    \n",
    "# model = init_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet101V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications import ResNet101V2\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Dropout, AveragePooling2D\n",
    "\n",
    "# def init_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(ResNet101V2(weights='imagenet', input_shape=(image_size, image_size, 3), include_top=False))\n",
    "#     # model.add(Flatten())\n",
    "\n",
    "#     model.add(AveragePooling2D(pool_size=(7,7)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128, 'relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "\n",
    "#     model.add(Dense(categories, 'softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = init_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# for _ in tqdm(range(10), desc='Traning'):\n",
    "#     model = init_model()\n",
    "#     model.fit(X_train, y_train, epochs=30, verbose=0)\n",
    "#     _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     result.append(acc)\n",
    "# print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, eval_accuracy = model.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams.update({'font.family': 'Arial', 'font.size' : 12})\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.ylim(bottom=0)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['accuracy', 'loss', 'val_accuracy', 'val_loss'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical back to integers\n",
    "y_test_true = np.array([np.where(x == x.max())[0][0] for x in y_test])\n",
    "y_test_pred = np.array([np.where(x == x.max())[0][0] for x in model.predict(X_test)])\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=unique_labels)\n",
    "cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays a sample of what the model got wrong\n",
    "visualizer([x for c, x in enumerate(X_test) if y_test_pred[c] != y_test_true[c]], [f'Predicted: {unique_labels[y[0]]}\\nActual: {unique_labels[y[1]]}' for c, y in enumerate(zip(y_test_pred, y_test_true)) if y_test_pred[c] != y_test_true[c]], grid=(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def center_box(box):\n",
    "    startX, startY, endX, endY = box.astype(\"int\")\n",
    "    h = endY - startY\n",
    "    w = endX - startX\n",
    "    aspect = h/w\n",
    "    if aspect > 1:\n",
    "        offset = int(np.round((h / 2) - (w / 2)))\n",
    "        return startX - offset, startY, endX + offset, endY\n",
    "    else:\n",
    "        offset = int(np.round((w / 2) - (h / 2)))\n",
    "        return startX, startY - offset, endX, endY + offset\n",
    "\n",
    "def classify_image(interpreter, image):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    input_data = image\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "# Face detect model\n",
    "prototxt_path = 'deploy.prototxt'\n",
    "weights_path = 'res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "face_detect = cv2.dnn.readNet(prototxt_path, weights_path)\n",
    "# Red, Green, Yellow\n",
    "color = [(0,0,255),(0,255,0),(128,128,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Classifier Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the current label (just in case)\n",
    "unique_labels = []\n",
    "[unique_labels.append(x) for x in label if x not in unique_labels]\n",
    "\n",
    "# Camera init\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Used for fps calculation\n",
    "time1 = 0\n",
    "time2 = 0\n",
    "\n",
    "# Main loop\n",
    "while(video_capture.isOpened()):\n",
    "    # Start timer for fps calculation\n",
    "    time1 = time.time()\n",
    "\n",
    "    # Read frame from camera\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print('Camera unavailable')\n",
    "        break\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # OpenCV DNN pre-processing\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (image_size, image_size))\n",
    "    # Process the pre-processed frame & find faces\n",
    "    face_detect.setInput(blob)\n",
    "    faces = face_detect.forward()\n",
    "\n",
    "    # For all faces detected in frame\n",
    "    for i in range(0, faces.shape[2]):\n",
    "        # Get the confidence that it is a face \n",
    "        confidence = faces[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            # Get coordinates for face\n",
    "            box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            startX, startY, endX, endY = center_box(box)\n",
    "\n",
    "            # ensure the bounding boxes fall within the dimensions of the frame\n",
    "            startX, startY = max(0, startX), max(0, startY)\n",
    "            endX, endY = min(w - 1, endX), min(h - 1, endY)\n",
    "\n",
    "            # Pre-process frame\n",
    "            frame_crop = frame[startY:endY, startX:endX]\n",
    "            frame_crop_resize = cv2.resize(frame_crop, (image_size, image_size), cv2.INTER_AREA)\n",
    "            frame_crop_resize_reshape = np.array(frame_crop_resize, dtype=np.float32).reshape(-1, image_size, image_size, 3)\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict(frame_crop_resize_reshape)\n",
    "\n",
    "            # Convert prediction to lables\n",
    "            pred_index = np.argmax(pred[0])\n",
    "\n",
    "            # Draw a rectangle around the faces with classification\n",
    "            text = unique_labels[pred_index] + ' (' +  str(round(pred[0][pred_index] * 100)) + '%)'\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color[pred_index], 2)\n",
    "            cv2.putText(frame, text, (startX, startY - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color[pred_index], 1, cv2.LINE_AA)\n",
    "\n",
    "    # Calculate fps\n",
    "    fps = 1/(time1 - time2)\n",
    "    time2 = time1\n",
    "    # Draw fps on frame\n",
    "    cv2.putText(frame, str(int(fps)) + 'fps', (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('window', frame)\n",
    "    \n",
    "    # Press ESC to quit\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  \n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Stage Classifier Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'model_3'\n",
    "model_hasmask = load_model(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the current label (just in case)\n",
    "unique_labels = []\n",
    "[unique_labels.append(x) for x in label if x not in unique_labels]\n",
    "\n",
    "# Takes a backup of the label\n",
    "unique_labels_backup = unique_labels.copy()\n",
    "\n",
    "# Camera init\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Used for fps calculation\n",
    "time1 = 0\n",
    "time2 = 0\n",
    "\n",
    "# Main loop\n",
    "while(video_capture.isOpened()):\n",
    "    # Start timer for fps calculation\n",
    "    time1 = time.time()\n",
    "\n",
    "    # Read frame from camera\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # OpenCV DNN pre-processing\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (image_size, image_size))\n",
    "    # Process the pre-processed frame & find faces\n",
    "    face_detect.setInput(blob)\n",
    "    faces = face_detect.forward()\n",
    "\n",
    "    # For all faces detected in frame\n",
    "    for i in range(0, faces.shape[2]):\n",
    "        # Get the confidence that it is a face \n",
    "        confidence = faces[0, 0, i, 2]\n",
    "        if confidence > 0.7:\n",
    "            # Get coordinates for face\n",
    "            box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            startX, startY, endX, endY = center_box(box)\n",
    "\n",
    "            # ensure the bounding boxes fall within the dimensions of the frame\n",
    "            startX, startY = max(0, startX), max(0, startY)\n",
    "            endX, endY = min(w - 1, endX), min(h - 1, endY)\n",
    "\n",
    "            # Pre-process frame\n",
    "            frame_crop = frame[startY:endY, startX:endX]\n",
    "            frame_crop_resize = cv2.resize(frame_crop, (image_size, image_size), cv2.INTER_AREA)\n",
    "            frame_crop_resize_reshape = np.array(frame_crop_resize, dtype=np.float32).reshape(-1, image_size, image_size, 3)\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model_hasmask.predict(frame_crop_resize_reshape)\n",
    "\n",
    "            # Sets label to first classifier\n",
    "            # NOTE: Make sure the labels actually match the category\n",
    "            unique_labels = ['no mask', 'mask']\n",
    "\n",
    "            # Convert prediction to lables\n",
    "            pred_index = np.argmax(pred[0])\n",
    "\n",
    "            if unique_labels[pred_index] == 'mask':\n",
    "                # Reassign the unique label\n",
    "                unique_labels = unique_labels_backup\n",
    "\n",
    "                # Make a prediction\n",
    "                pred = model.predict(frame_crop_resize_reshape)\n",
    "\n",
    "                # Convert prediction to lables\n",
    "                pred_index = np.argmax(pred[0])\n",
    "\n",
    "            # Draw a rectangle around the faces with classification\n",
    "            text = unique_labels[pred_index] + ' (' +  str(round(pred[0][pred_index] * 100)) + '%)'\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), color[pred_index], 2)\n",
    "            cv2.putText(frame, text, (startX, startY - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color[pred_index], 1, cv2.LINE_AA)\n",
    "\n",
    "    # Calculate fps\n",
    "    fps = 1/(time1 - time2)\n",
    "    time2 = time1\n",
    "    # Draw fps on frame\n",
    "    cv2.putText(frame, str(int(fps)) + 'fps', (0, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('window', frame)\n",
    "    \n",
    "    # Press ESC to quit\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  \n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_name) # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(model_name + '.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc8db8338442da81e46c52f4a512a049d88bdbadd496774bc741b7bd95e52413"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
